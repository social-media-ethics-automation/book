{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fb9e2fa-5f32-4ddc-bf9c-5256d8e5076a",
   "metadata": {},
   "source": [
    "# A3: Political Bias\n",
    "In this assignment, you will be trying to calcualate the political bias and reliability of tweets in tweet searches. \n",
    "\n",
    "To do this, you will use some website bias and reliability ratings based on the Media Bias Chart (https://adfontesmedia.com/static-mbc). When you find tweets that link to a website, if you have a rating for it you will find the bias and reliability ratings, and at the end you will report the average bias and average reliability of the tweets in your search.\n",
    "\n",
    "There are, of course, many problems with trying to calculate political bias and reliability in this way, which you will answer questions about after the programming section of the assignment.\n",
    "\n",
    "In the programming portion, there are many pieces already created for you, but there are labeled sections where you will have to write the code yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef2034-dcde-40b6-9329-b446a0e931f5",
   "metadata": {},
   "source": [
    "### Our usual twitter set-up code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9b9f36-4fe8-4852-adf5-1135f70c7e41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\python310\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\python310\\lib\\site-packages (from tweepy) (2.27.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\python310\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\python310\\lib\\site-packages (from tweepy) (3.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python310\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.12)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# make sure tweepy library is installed\n",
    "!pip install tweepy \n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b7be23-dfc0-43e6-a049-acd686dc3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my twitter keys\n",
    "import my_bot_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078b3e41-1c0e-4fad-bc4d-9c1fd0c28a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log into tweepy\n",
    "client = tweepy.Client(\n",
    "    bearer_token=my_bot_keys.bearer_token,\n",
    "    consumer_key=my_bot_keys.consumer_key, consumer_secret=my_bot_keys.consumer_secret,                   \n",
    "    access_token=my_bot_keys.access_token, access_token_secret=my_bot_keys.access_token_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6f37f3-a7eb-494f-8328-13870f918a22",
   "metadata": {},
   "source": [
    "### Install libraries for handling website urls and for loading csvs\n",
    "You will need `URLExtract` and `urllib3` to find website urls in tweet text, and turn shortened urls (like https://bit.ly/3gE1Nsl) into the full urls where you can tell what news organization (if any) it points to.\n",
    "\n",
    "You will also need `pandas` to load the csv data file of media website bias and reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b679ff0-318b-4ce0-939b-df4bb8f491eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: URLExtract in c:\\python310\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: idna in c:\\python310\\lib\\site-packages (from URLExtract) (3.3)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from URLExtract) (3.7.1)\n",
      "Requirement already satisfied: uritools in c:\\python310\\lib\\site-packages (from URLExtract) (4.0.0)\n",
      "Requirement already satisfied: platformdirs in c:\\python310\\lib\\site-packages (from URLExtract) (2.5.2)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in c:\\python310\\lib\\site-packages (1.26.9)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01murlreq\u001b[39;00m\n\u001b[0;32m      5\u001b[0m extractor \u001b[38;5;241m=\u001b[39m URLExtract()\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "!pip install URLExtract\n",
    "!pip install urllib3\n",
    "from urlextract import URLExtract\n",
    "import urllib.request as urlreq\n",
    "extractor = URLExtract()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2dbd35-52bf-4b5b-943d-3485ef7bad6a",
   "metadata": {},
   "source": [
    "### Load the media site info\n",
    "Our measure of media website bias and reliability is based on the Media Bias Chart 9.0 (https://adfontesmedia.com/static-mbc)\n",
    "\n",
    "I have made a csv (comma separated varaible) file with some of the sites shown on that chart. I made my own simplification based on the grid in the graphic with bias ranging from -4 to +4, and reliability from -4 to +2 as follows:\n",
    "![Media bias chart with grid, showing the range labels](./bias_chart_divisions.jpg)\n",
    "\n",
    "These are saved in the file `media_info.csv`, where each row of text is the info for one media site. For example:\n",
    "`wsws.org,-3,-1`\n",
    "Means that the site wsws.org has a bias of -3 (Hyper-Partisan Left) and a reliability of -1 (Opinion OR High Variation in Reliability). \n",
    "\n",
    "If you want to modify the `media_info.csv` file, you can open it by right-clicking and selecting Open With -> Editor. You can then modify entries, or add new ones for more sites (like from the interactive media bias chart here: https://adfontesmedia.com/interactive-media-bias-chart/). Then save it and rerun this code and the code below.\n",
    "\n",
    "![Show context menues for right clicking the csv file, then choosing Open With, then choosing Editor](./open_csv_editor.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789def50-4d8a-493b-a8fd-e4c19f677011",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_info_df = pd.read_csv('media_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb7a43-226d-4117-bda1-44c62feb1ac8",
   "metadata": {},
   "source": [
    "### Now reorganize the information for easier use\n",
    "Get a list of just the media websites in the variable `media_sites`\n",
    "\n",
    "Make a dictionary where you can look up the bias for a site in the variable `media_bias_lookup`\n",
    "\n",
    "Make a dictionary where you can look up the reliability for a site in the variable `media_reliability_lookup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441e424-dc6f-40cc-8ecb-b71705c7ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_sites = media_info_df['site']\n",
    "\n",
    "media_bias_lookup = {m_info['site']: m_info['bias'] for i, m_info in media_info_df.iterrows()}\n",
    "media_reliability_lookup = {m_info['site']: m_info['reliability'] for i, m_info in media_info_df.iterrows()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53630c-e86f-48c2-9fe6-41cc1a0709eb",
   "metadata": {},
   "source": [
    "### Demo of looking up media site info\n",
    "This is a demo of how to get information from the variables from the code above. You will need to access this info yourself below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89375c8-f60f-449f-a5ea-c616870f5867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# media_sites has a list of sites to look for in the tweets\n",
    "# you can loop through them like this\n",
    "for site in media_sites:\n",
    "    print(site, end=', ')\n",
    "\n",
    "print()\n",
    "\n",
    "# media_bias_lookup lets you look up the bias for a site from the media_sites list\n",
    "print(media_bias_lookup[\"pbs.org\"])\n",
    "\n",
    "# media_reliability_lookup lets you look up the reliability for a site from the media_sites list\n",
    "print(media_reliability_lookup[\"pbs.org\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec218713-6d9a-4c3b-b4e1-4ea1c70dfda0",
   "metadata": {},
   "source": [
    "### Utility functions: find all the urls in the text, and turn shortened urls into full urls\n",
    "\n",
    "The function `follow_url_redirects` takes a potentially shortened url (like https://bit.ly/3gE1Nsl), and turns it into the full url with the news site. Note: You do not need to use this function directly.\n",
    "\n",
    "The function `find_expanded_urls` takes a piece of text that has urls in it, and returns a list of the expanded version of all those urls. Note: You **will** have to use this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f117fd97-7ae8-4c0e-b1a5-d74ed1bf2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def follow_url_redirects(url):\n",
    "    still_redirecting = True\n",
    "    new_url = url\n",
    "    status_code = 301\n",
    "    num_tries = 0\n",
    "    while still_redirecting and num_tries < 4:\n",
    "        try:\n",
    "            r = requests.get(new_url, allow_redirects=False, timeout=1)\n",
    "            status_code = r.status_code\n",
    "            if 'Location' in r.headers: \n",
    "                new_url = r.headers['Location']\n",
    "            if status_code == 301 and 'Location' in r.headers:\n",
    "                still_redirecting = True\n",
    "            else:\n",
    "                still_redirecting = False\n",
    "        except BaseException as err:\n",
    "            print(\"-- Warning: could not load url: \" + url + \", err: \" + str(err))\n",
    "            still_redirecting = False\n",
    "        \n",
    "    return new_url\n",
    "\n",
    "def find_expanded_urls(piece_of_text):\n",
    "    urls = extractor.find_urls(piece_of_text)\n",
    "    fullURLs = []\n",
    "    for url in urls:\n",
    "        fullURL = follow_url_redirects(url)\n",
    "        fullURLs.append(fullURL)\n",
    "    return fullURLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32407a97-c12f-48b2-bf3e-14f5e6f734bd",
   "metadata": {},
   "source": [
    "### Demo of how to use the function `find_expanded_urls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18913522-4526-40ad-a652-2b5d116b1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = find_expanded_urls(\"An article on misuse of facial recognition https://bit.ly/3gE1Nsl. a documentary on bias and algorithms: https://bit.ly/3Lr0Xx7\")\n",
    "\n",
    "for url in urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79e13fb-3674-49d0-bb55-0b20887d8dd4",
   "metadata": {},
   "source": [
    "# Your Code!\n",
    "### Create functions to look up bias an reliability for a website\n",
    "You need to create two functions: `find_bias` and `find_reliability`\n",
    "\n",
    "Note: These two functions should look very similar. \n",
    "\n",
    "* They should each take as an argument a url. \n",
    "* They should then do a for loop over all the sites in the `media_sites` list. \n",
    "    * If that site string (e.g., \"reuters.com\") appears in the given url string (e.g, \"https://www.reuters.com/lifestyle/sports/...\"), then the function should return the bias or reliability for that site in the `media_bias_lookup`/`media_reliability_lookup` dictionary. \n",
    "* If none of the media_sites sites is found in the url, then no need to do anything as the function will by default return \"None\".\n",
    "\n",
    "Suggestion: write the code first not in a function, and then turn it into a function at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa9fb9-2632-42e0-8832-aa5fa1a1143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c2e28-ce05-4414-a6c7-d6431cc2744b",
   "metadata": {},
   "source": [
    "### Test your code for find_bias and find_reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd060d2-1cb0-4f74-ad87-0cd272a0f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube isn't in the list, so should return None for bias\n",
    "print(find_bias(\"https://www.youtube.com/watch?v=JTfhYyTuT44\"))\n",
    "\n",
    "# reuters should return -1 for bias\n",
    "print(find_bias(\"https://www.reuters.com/lifestyle/sports/alpine-skiing-womens-downhill-pushed-back-by-least-30-mins-due-wind-2022-02-15/\"))\n",
    "\n",
    "# reuters should return 2 for reliability\n",
    "print(find_reliability(\"https://www.reuters.com/lifestyle/sports/alpine-skiing-womens-downhill-pushed-back-by-least-30-mins-due-wind-2022-02-15/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535b093e-3e7a-486e-bb05-fbfcfbe9ab1c",
   "metadata": {},
   "source": [
    "# Your Code!\n",
    "### Create a function that, for a given query, displays the average bias and reliability for sites linked to from the found tweets\n",
    "\n",
    "Create a function `find_twitter_bias_and_reliability` that takes a query and optionally max_results (by default 10). You can also make an optional `show_progress` argument as well if you want (see lecture 12 demo).\n",
    "\n",
    "What your function should do: \n",
    "\n",
    "Do a twitter search (you can do our normal `search_recent_tweets`, or if you want, you can experiment with `search_all_tweets`).\n",
    "\n",
    "Create some variables to track the total number of links you were able to evaluate for bias/reliabilty, and the total sum of the bias and the total sum of the reliability you encounter with those links.\n",
    "\n",
    "Next loop through all the tweets in your search.\n",
    "\n",
    "For each tweet, use the `find_expanded_urls` function to get all the expanded urls in the tweet. Also, print the tweet text/link.\n",
    "\n",
    "For each url in those urls:\n",
    "* try to find the bias and the reliability of the url (using your `find_bias` and `find_reliability`)\n",
    "* If you were able to find the bias and reliability\n",
    "  * add to the count of urls you were able to evaluate\n",
    "  * add the bias to the total bias you found from links\n",
    "  * add the reliability to the total reliability found from your links\n",
    "  * Also, print the url and the bias/responsibility info\n",
    "* If you were not able to find the bias and reliability\n",
    "  * Print the url and say you weren't able to evaluate it.\n",
    "  \n",
    "After all of that, print out the total number of urls you were able to evaluate.\n",
    "\n",
    "If you were able to evaluate at least 1 url, then print out the average bias and average reliability for all the urls in your search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e03b95-f18f-47d9-855a-b8a824d8a57c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313fc871-3815-4c34-9287-92e89f3cf1b9",
   "metadata": {},
   "source": [
    "### Test of your code \n",
    "This should test your code with a search for \"senate,\" making sure it has a link to some website, that it doesn't link to twitter (many tweets refer to other tweets), and that it isn't a retweet. Also, have it request 100 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4400e41-0098-4a7e-a850-f95aad5f3623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = 'senate has:links -is:retweet -url:twitter.com'\n",
    "find_twitter_bias_and_reliability(query, max_results=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b214e53-071c-4eea-80d0-253c56618648",
   "metadata": {},
   "source": [
    "# Your Code!\n",
    "Do three more searches of your own choosing which give at least some urls that your code can evaluate. \n",
    "\n",
    "You can search for different news topics, political figures, or other interests. You can also search for tweets from a particular user (though if you are using search_recent_tweets, it only gets their tweets from the last 7 days)\n",
    "\n",
    "You can also try searching for phrases used specifically by different groups. For example, if you want searches that are more likely to turn up unreliable or conspiracy theory sites, you can try searches like: \n",
    "* Soros \n",
    "* \"covid cult\" \n",
    "* #Censorship \n",
    "* shitlibs\n",
    "\n",
    "You can do other modifications to your search as well (like restricting more websites in the search than just twitter.com).\n",
    "\n",
    "After each search there are some questions to answer. And some final reflection questions at the end as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bebeb99-20d7-4cdc-bd9c-85f99785a70c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Search 1 code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de7bec-e255-4bdf-b58c-e4e074c4962f",
   "metadata": {},
   "source": [
    "### Search 1 follow-up questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38968a41-f5aa-43c7-8ecb-714bf27d712f",
   "metadata": {},
   "source": [
    "Write at least 2 sentences in response to each of these questions (you can edit this text):\n",
    "\n",
    "* Looking at your calculated average bias and reliability, and skimming through the tweets, how well do you think those averages represent the tweets in the search?\n",
    "\n",
    "* Are there particular sites that showed up a lot that your program wasn't able to find bias/reliability websites for? How hard do you think it would be to evaluate those sites too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0bc016-185b-41a8-954b-40260650e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Search 2 code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19220e45-df37-4ee1-8bd1-2005f40d28fb",
   "metadata": {},
   "source": [
    "### Search 2 follow-up questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25be8c0-4497-4d97-a720-935eef8a3507",
   "metadata": {},
   "source": [
    "Write at least 2 sentences in response to each of these questions (you can edit this text):\n",
    "\n",
    "* Looking at your calculated average bias and reliability, and skimming through the tweets, how well do you think those averages represent the tweets in the search?\n",
    "\n",
    "* Are there particular sites that showed up a lot that your program wasn't able to find bias/reliability websites for? How hard do you think it would be to evaluate those sites too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffde8e9-c59d-411e-b258-cf754c50c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Search 3 code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6eb6b9-f094-4cce-b980-d60a0e483b9d",
   "metadata": {},
   "source": [
    "### Search 3 follow-up questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2aa149-8926-4887-952e-222517765dca",
   "metadata": {},
   "source": [
    "Write at least 2 sentences in response to each of these questions (you can edit this text):\n",
    "\n",
    "* Looking at your calculated average bias and reliability, and skimming through the tweets, how well do you think those averages represent the tweets in the search?\n",
    "\n",
    "* Are there particular sites that showed up a lot that your program wasn't able to find bias/reliability websites for? How hard do you think it would be to evaluate those sites too?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef9aa0-9dc4-40ff-9ede-ebf339a117ab",
   "metadata": {},
   "source": [
    "### Final Reflection questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87032f05-e4a7-4875-9411-1ae97c350820",
   "metadata": {},
   "source": [
    "Write at least 2 sentences in response to each of these questions (you can edit this text):\n",
    "\n",
    "* In what ways do you feel like the info from the Media Bias Chart works well?\n",
    "\n",
    "* In what ways do you feel like the info from the Media Bias Chart works poorly?\n",
    "\n",
    "* If you could redesign the Media Bias Chart, what would you want to do (e.g., add some other dimension besides just bias/responsibility, change how it is evaluated, add more news sources, consider different countries)?\n",
    "\n",
    "* If you were able to run your `find_twitter_bias_and_reliability` function on the whole history of a twitter user (e.g, thousands of tweets), how accurately do you think you'd be able to judge that users' political views and susceptibility to consipracy theories?\n",
    "\n",
    "* What might a social media company or an advertizer (including political campaigns) want to do with information on a users' political views and susceptibility to consipracy theories?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}